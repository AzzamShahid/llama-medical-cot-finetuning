{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Fine-tuning LLaMA 3.2 (3B) on Medical Chain-of-Thought Dataset","metadata":{}},{"cell_type":"markdown","source":"## Task Overview","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we fine-tune the LLaMA 3.2 (3B) model on a medical Chain-of-Thought (CoT) dataset using parameter-efficient fine-tuning (PEFT) with Unsloth. The goal is to improve the model's ability to generate step-by-step medical reasoning with structured responses.","metadata":{}},{"cell_type":"markdown","source":"## 1. Environment Setup","metadata":{}},{"cell_type":"markdown","source":"### Install Required Libraries","metadata":{}},{"cell_type":"markdown","source":"First, let's install required dependencies:","metadata":{}},{"cell_type":"code","source":"# Install unsloth and unsloth-zoo from GitHub\n!pip install --upgrade --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git git+https://github.com/unslothai/unsloth-zoo.git\n# Install required dependencies\n!pip install bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n!pip install transformers==4.51.3\n!pip install unsloth\n!pip install rouge_score evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:46:16.484301Z","iopub.execute_input":"2025-05-31T00:46:16.484551Z","iopub.status.idle":"2025-05-31T00:48:10.075988Z","shell.execute_reply.started":"2025-05-31T00:46:16.484532Z","shell.execute_reply":"2025-05-31T00:48:10.074698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###   Import Dependencies","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport random\nimport numpy as np\nfrom datasets import load_dataset\nimport nltk\nfrom unsloth import FastLanguageModel\nimport wandb\nfrom rouge_score import rouge_scorer\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:48:10.077774Z","iopub.execute_input":"2025-05-31T00:48:10.078129Z","iopub.status.idle":"2025-05-31T00:48:49.863848Z","shell.execute_reply.started":"2025-05-31T00:48:10.078092Z","shell.execute_reply":"2025-05-31T00:48:49.863112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###   Set Random Seeds for Reproducibility","metadata":{}},{"cell_type":"code","source":"random.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:48:49.864587Z","iopub.execute_input":"2025-05-31T00:48:49.865212Z","iopub.status.idle":"2025-05-31T00:48:49.875633Z","shell.execute_reply.started":"2025-05-31T00:48:49.865191Z","shell.execute_reply":"2025-05-31T00:48:49.874685Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Download nltk data","metadata":{}},{"cell_type":"code","source":"nltk.download('punkt', quiet=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:48:49.877084Z","iopub.execute_input":"2025-05-31T00:48:49.877347Z","iopub.status.idle":"2025-05-31T00:48:50.078120Z","shell.execute_reply.started":"2025-05-31T00:48:49.877331Z","shell.execute_reply":"2025-05-31T00:48:50.076353Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###   Check GPU availability","metadata":{}},{"cell_type":"code","source":"print(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nelse:\n    print(\"No GPU detected. This notebook requires GPU acceleration.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:48:50.079172Z","iopub.execute_input":"2025-05-31T00:48:50.079491Z","iopub.status.idle":"2025-05-31T00:48:50.085858Z","shell.execute_reply.started":"2025-05-31T00:48:50.079464Z","shell.execute_reply":"2025-05-31T00:48:50.085015Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Configure Weights & Biases Authentication","metadata":{}},{"cell_type":"code","source":"try:\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\nsecret_value_1 = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n    \n    os.environ[\"WANDB_API_KEY\"] = secret_value_1\n    os.environ[\"WANDB_PROJECT\"] = \"llama-medical-cot\"\n    os.environ[\"WANDB_WATCH\"] = \"gradients\"\n    \n    wandb.login(key=wandb_api_key)\n    print(\"Logged in to Weights & Biases successfully!\")\n    wandb_enabled = True\nexcept Exception as e:\n    print(f\"Warning: Failed to log in to Weights & Biases. Error: {e}\")\n    print(\"Training will continue but metrics won't be logged to W&B.\")\n    wandb_enabled = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:48:50.086663Z","iopub.execute_input":"2025-05-31T00:48:50.086948Z","iopub.status.idle":"2025-05-31T00:48:56.205235Z","shell.execute_reply.started":"2025-05-31T00:48:50.086929Z","shell.execute_reply":"2025-05-31T00:48:56.204529Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Dataset Preparation","metadata":{}},{"cell_type":"markdown","source":"###  Load Medical Chain-of-Thought Dataset\n","metadata":{}},{"cell_type":"code","source":"print(\"Loading dataset...\")\n\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:48:56.206041Z","iopub.execute_input":"2025-05-31T00:48:56.206590Z","iopub.status.idle":"2025-05-31T00:48:59.757743Z","shell.execute_reply.started":"2025-05-31T00:48:56.206568Z","shell.execute_reply":"2025-05-31T00:48:59.757219Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Display Dataset Information","metadata":{}},{"cell_type":"code","source":"print(dataset)\nprint(f\"Number of training examples: {len(dataset['train'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:48:59.758411Z","iopub.execute_input":"2025-05-31T00:48:59.758600Z","iopub.status.idle":"2025-05-31T00:48:59.762808Z","shell.execute_reply.started":"2025-05-31T00:48:59.758586Z","shell.execute_reply":"2025-05-31T00:48:59.762242Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Show Sample Data","metadata":{}},{"cell_type":"code","source":"print(\"\\nSample from the dataset:\")\nsample = dataset[\"train\"][0]\nprint(f\"Question: {sample['Question']}\")\nprint(f\"Response: {sample['Response']}\")\nprint(f\"Complex_CoT: {sample['Complex_CoT'][:200]}...\" if len(sample['Complex_CoT']) > 200 else sample['Complex_CoT'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:48:59.763419Z","iopub.execute_input":"2025-05-31T00:48:59.763624Z","iopub.status.idle":"2025-05-31T00:48:59.842440Z","shell.execute_reply.started":"2025-05-31T00:48:59.763603Z","shell.execute_reply":"2025-05-31T00:48:59.841772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Shuffle Dataset","metadata":{}},{"cell_type":"code","source":"train_data = dataset[\"train\"].shuffle(seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:48:59.844819Z","iopub.execute_input":"2025-05-31T00:48:59.845027Z","iopub.status.idle":"2025-05-31T00:48:59.866781Z","shell.execute_reply.started":"2025-05-31T00:48:59.844991Z","shell.execute_reply":"2025-05-31T00:48:59.866243Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Split Dataset (100 validation, rest training)","metadata":{}},{"cell_type":"code","source":"val_data = train_data.select(range(100))  # Exactly 100 rows as required\ntrain_data = train_data.select(range(100, len(train_data)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:48:59.867491Z","iopub.execute_input":"2025-05-31T00:48:59.867723Z","iopub.status.idle":"2025-05-31T00:48:59.880082Z","shell.execute_reply.started":"2025-05-31T00:48:59.867707Z","shell.execute_reply":"2025-05-31T00:48:59.879353Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Format Data for Chain-of-Thought Training","metadata":{}},{"cell_type":"code","source":"def format_medical_cot(example):\n    \"\"\"Format data into prompt and completion pairs with CoT structure\"\"\"\n    prompt = f\"\"\"Below is a medical question. Think step by step to solve it.\nQuestion: {example['Question']}\n\"\"\"\n    \n    completion = f\"\"\"<think>\n{example['Complex_CoT']}\n</think>\n<response>\n{example['Response']}\n</response>\"\"\"\n    \n    return {\"text\": prompt + completion}\n\n# Format the datasets\nprint(\"Formatting datasets...\")\ntrain_formatted = train_data.map(format_medical_cot, remove_columns=train_data.column_names)\nval_formatted = val_data.map(format_medical_cot, remove_columns=val_data.column_names)\n\nprint(f\"Training samples formatted: {len(train_formatted)}\")\nprint(f\"Validation samples formatted: {len(val_formatted)}\")\n\n# Show sample\nprint(\"\\nSample formatted data:\")\nprint(train_formatted[0][\"text\"][:300] + \"...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:48:59.880922Z","iopub.execute_input":"2025-05-31T00:48:59.881183Z","iopub.status.idle":"2025-05-31T00:49:02.170763Z","shell.execute_reply.started":"2025-05-31T00:48:59.881160Z","shell.execute_reply":"2025-05-31T00:49:02.169984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Model Setup with Unsloth","metadata":{}},{"cell_type":"markdown","source":"###  Import Unsloth FastLanguageModel","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel, UnslothTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:49:02.171667Z","iopub.execute_input":"2025-05-31T00:49:02.171853Z","iopub.status.idle":"2025-05-31T00:49:02.175216Z","shell.execute_reply.started":"2025-05-31T00:49:02.171838Z","shell.execute_reply":"2025-05-31T00:49:02.174475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load LLaMA 3.2 3B Model with 4-bit Quantization","metadata":{}},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n            model_name=\"unsloth/llama-3.2-3b-Instruct\",\n            max_seq_length=2048,  # Reduced from 4096 to save memory\n            dtype=None,  # None for auto detection. float16, bfloat16, or float32\n            load_in_4bit=True,  # Use 4-bit quantization to save memory\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:49:02.176025Z","iopub.execute_input":"2025-05-31T00:49:02.176302Z","iopub.status.idle":"2025-05-31T00:49:40.554468Z","shell.execute_reply.started":"2025-05-31T00:49:02.176276Z","shell.execute_reply":"2025-05-31T00:49:40.553679Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Configure LoRA (Parameter-Efficient Fine-tuning)","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n            model,\n            r=16,             # Rank of the LoRA adapters\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n                          \"gate_proj\", \"up_proj\", \"down_proj\"], \n            lora_alpha=16,    # Alpha parameter for LoRA scaling\n            lora_dropout=0, # Dropout probability for LoRA layers\n            bias=\"none\",      # Add bias to LoRA adapters\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:49:40.555356Z","iopub.execute_input":"2025-05-31T00:49:40.556043Z","iopub.status.idle":"2025-05-31T00:49:44.016570Z","shell.execute_reply.started":"2025-05-31T00:49:40.555993Z","shell.execute_reply":"2025-05-31T00:49:44.015982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Display Trainable Parameters","metadata":{}},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.4f}% of total)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:49:44.017289Z","iopub.execute_input":"2025-05-31T00:49:44.017492Z","iopub.status.idle":"2025-05-31T00:49:44.031062Z","shell.execute_reply.started":"2025-05-31T00:49:44.017476Z","shell.execute_reply":"2025-05-31T00:49:44.030250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Data already prepared for Unsloth training...\")\nprint(f\"Training samples: {len(train_formatted)}\")\nprint(f\"Validation samples: {len(val_formatted)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T00:49:44.031769Z","iopub.execute_input":"2025-05-31T00:49:44.032053Z","iopub.status.idle":"2025-05-31T00:49:44.048818Z","shell.execute_reply.started":"2025-05-31T00:49:44.032026Z","shell.execute_reply":"2025-05-31T00:49:44.048167Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. TRAINING CONFIGURATION","metadata":{}},{"cell_type":"markdown","source":"### Configure Training Arguments","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\nimport datetime\n\n# Get a unique run name for wandb\nrun_name = f\"llama3.2-3b-medical-cot-{datetime.datetime.now().strftime('%Y%m%d-%H%M')}\"\n\n# Set up optimized training arguments for faster training\ntraining_args = TrainingArguments(\n    output_dir=f\"./results/{run_name}\",\n    num_train_epochs=1,  # Changed to 1 epoch as requested\n    per_device_train_batch_size=2,  # Increased from 1 to 2 (safer than 4)\n    per_device_eval_batch_size=2,   # Increased from 1 to 2\n    gradient_accumulation_steps=8,  # Reduced from 16 to 8 (effective batch size still 32)\n    learning_rate=5e-4,  # Higher learning rate for faster convergence\n    weight_decay=0.01,\n    logging_steps=25,    # Reduced logging frequency to save time\n    eval_steps=500,      # Much less frequent evaluation\n    save_steps=1000,     # Much less frequent saving\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    report_to=\"wandb\" if wandb_enabled else \"none\",\n    run_name=run_name,\n    save_total_limit=1,  # Reduced from 2 to save disk space\n    fp16=True,\n    warmup_steps=25,     # Increased warmup for better convergence in 1 epoch\n    gradient_checkpointing=True,   # Enable gradient checkpointing to save memory\n)\n\nprint(\"Training arguments configured successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T01:09:10.945787Z","iopub.execute_input":"2025-05-31T01:09:10.946188Z","iopub.status.idle":"2025-05-31T01:09:10.988083Z","shell.execute_reply.started":"2025-05-31T01:09:10.946158Z","shell.execute_reply":"2025-05-31T01:09:10.987359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Initialize Trainer and Start Fine-tuning","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\n\n# Configure tokenizer\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Initialize wandb if enabled\nif wandb_enabled:\n    if wandb.run is not None:\n        wandb.finish()\n    wandb.init(project=\"llama-medical-cot\", name=run_name)\n\n# Create trainer with optimized settings\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_formatted,\n    eval_dataset=val_formatted,\n    tokenizer=tokenizer,\n    dataset_text_field=\"text\",\n    max_seq_length=2048,  # Keep original value\n    packing=False,        # Keep original value\n)\n\nprint(\"Starting fine-tuning...\")\ntrain_result = trainer.train()\nprint(\"Training complete!\")","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-05-31T01:09:13.933505Z","iopub.execute_input":"2025-05-31T01:09:13.933776Z","iopub.status.idle":"2025-05-31T07:23:13.977975Z","shell.execute_reply.started":"2025-05-31T01:09:13.933756Z","shell.execute_reply":"2025-05-31T07:23:13.977303Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. EVALUATION AND SAVING","metadata":{}},{"cell_type":"markdown","source":"### Evaluate Model and Save Locally","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport os\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\n# Get secrets from Kaggle\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\n# Login to Hugging Face\nlogin(token=hf_token)\n\n# Your existing model training code here...\n# (Include your full training pipeline)\n\n# Evaluate model\nprint(\"Evaluating model...\")\neval_results = trainer.evaluate()\nprint(\"Evaluation results:\")\nfor key, value in eval_results.items():\n    if isinstance(value, (int, float)):\n        print(f\"  {key}: {value:.4f}\")\n\n# Save model locally in Kaggle environment\nprint(\"Saving model...\")\nlocal_path = \"/kaggle/working/llama-3b-medical-cot\"\nmodel.save_pretrained(local_path)\ntokenizer.save_pretrained(local_path)\nprint(f\"Model saved locally to {local_path}\")\n\n# Upload to Hugging Face Hub\nprint(\"Uploading to Hugging Face Hub...\")\nrepo_name = \"AzzamShahid/llama-3b-medical-cot\"  # Your HuggingFace username\n\ntry:\n    model.push_to_hub(repo_name, token=hf_token)\n    tokenizer.push_to_hub(repo_name, token=hf_token)\n    print(f\"Model successfully uploaded to: https://huggingface.co/{repo_name}\")\nexcept Exception as e:\n    print(f\"Error uploading to Hub: {e}\")\n    print(\"Model is saved locally and can be downloaded from Kaggle output\")\n\n# Optional: Save evaluation results\nimport json\nwith open(\"/kaggle/working/eval_results.json\", \"w\") as f:\n    json.dump(eval_results, f, indent=2)\nprint(\"Evaluation results saved to eval_results.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T07:36:39.890114Z","iopub.execute_input":"2025-05-31T07:36:39.890496Z","iopub.status.idle":"2025-05-31T07:37:49.050212Z","shell.execute_reply.started":"2025-05-31T07:36:39.890474Z","shell.execute_reply":"2025-05-31T07:37:49.049548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define Inference Function","metadata":{}},{"cell_type":"code","source":"def generate_medical_answer(question, model=None, tokenizer=None):\n    \"\"\"Generate a medical answer with CoT reasoning using the model\"\"\"\n    \n    try:\n        # Format the prompt to match training format\n        prompt = f\"\"\"Below is a medical question. Think step by step to solve it.\n\nQuestion: {question}\n\nAnswer: Let me think through this step by step.\n\n\"\"\"\n        \n        # Prepare model for inference\n        FastLanguageModel.for_inference(model)\n        \n        # Tokenize input\n        inputs = tokenizer(\n            [prompt], \n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=512  # Prevent overly long inputs\n        )\n        \n        # Move to appropriate device (cuda if available)\n        device = next(model.parameters()).device\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        # Generate response\n        with torch.no_grad():  # Save memory during inference\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=512,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95,\n                top_k=40,\n                repetition_penalty=1.1,\n                use_cache=True,\n                pad_token_id=tokenizer.eos_token_id,  # Handle padding\n                eos_token_id=tokenizer.eos_token_id\n            )\n        \n        # Decode only the new tokens (exclude the input prompt)\n        input_length = inputs['input_ids'].shape[1]\n        new_tokens = outputs[0][input_length:]\n        response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n        \n        return response.strip()\n    \n    except Exception as e:\n        return f\"Error generating response: {str(e)}\"\n\n# Test the fine-tuned model\ndef test_model(model, tokenizer):\n    \"\"\"Test the model with sample medical questions\"\"\"\n    \n    test_questions = [\n        \"What are potential causes of acute chest pain, and how would you differentiate between them?\",\n        \"A 65-year-old patient presents with shortness of breath and ankle swelling. What should be considered in the differential diagnosis?\",\n        \"What are the key differences between Type 1 and Type 2 diabetes mellitus?\"\n    ]\n    \n    print(\"Testing fine-tuned medical model:\")\n    print(\"=\" * 50)\n    \n    for i, question in enumerate(test_questions, 1):\n        print(f\"\\nTest {i}:\")\n        print(f\"Question: {question}\")\n        print(\"-\" * 30)\n        \n        response = generate_medical_answer(question, model, tokenizer)\n        print(f\"Model Response:\\n{response}\")\n        print(\"=\" * 50)\n\n# Run test if training was successful\nif 'model' in locals() and 'tokenizer' in locals():\n    try:\n        test_model(model, tokenizer)\n    except Exception as e:\n        print(f\"Error during testing: {e}\")\nelse:\n    print(\"Model or tokenizer not available for testing\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T07:38:51.945840Z","iopub.execute_input":"2025-05-31T07:38:51.946497Z","iopub.status.idle":"2025-05-31T07:40:17.714474Z","shell.execute_reply.started":"2025-05-31T07:38:51.946471Z","shell.execute_reply":"2025-05-31T07:40:17.713796Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. SUMMARY","metadata":{}},{"cell_type":"markdown","source":"###  Display Training Summary","metadata":{}},{"cell_type":"code","source":"print(\"\\n==== Training Summary ====\")\n\n# Check dataset\ntry:\n    if 'train_formatted' in locals() and 'val_formatted' in locals():\n        if len(train_formatted) > 0 and len(val_formatted) > 0:\n            print(\"✅ Dataset loaded and formatted successfully\")\n            print(f\"   Training samples: {len(train_formatted)}\")\n            print(f\"   Validation samples: {len(val_formatted)}\")\n        else:\n            print(\"❌ Dataset formatted but empty\")\n    else:\n        print(\"❌ Dataset formatting failed\")\nexcept Exception as e:\n    print(f\"❌ Dataset loading failed: {str(e)}\")\n\n# Check model\ntry:\n    if 'model' in locals() and model is not None:\n        print(\"✅ Model loaded successfully\")\n        print(f\"   Model type: {type(model).__name__}\")\n    else:\n        print(\"❌ Model loading failed\")\nexcept Exception as e:\n    print(f\"❌ Model loading failed: {str(e)}\")\n\n# Check tokenizer\ntry:\n    if 'tokenizer' in locals() and tokenizer is not None:\n        print(\"✅ Tokenizer loaded successfully\")\n    else:\n        print(\"❌ Tokenizer loading failed\")\nexcept Exception as e:\n    print(f\"❌ Tokenizer loading failed: {str(e)}\")\n\n# Check training\ntry:\n    if 'train_result' in locals() and train_result is not None:\n        print(\"✅ Training completed successfully\")\n        if hasattr(train_result, 'metrics'):\n            final_loss = train_result.metrics.get('train_loss', 'N/A')\n            if final_loss != 'N/A':\n                print(f\"   Final training loss: {final_loss:.4f}\")\n            else:\n                print(\"   Training loss: Not available\")\n        else:\n            print(\"   Training metrics: Not available\")\n    else:\n        print(\"❌ Training not completed\")\nexcept Exception as e:\n    print(f\"❌ Training check failed: {str(e)}\")\n\n# Check evaluation\ntry:\n    if 'eval_results' in locals() and eval_results is not None:\n        print(\"✅ Evaluation completed successfully\")\n        # Display key evaluation metrics\n        if isinstance(eval_results, dict):\n            eval_loss = eval_results.get('eval_loss', 'N/A')\n            if eval_loss != 'N/A':\n                print(f\"   Evaluation loss: {eval_loss:.4f}\")\n            \n            # Check for other common metrics\n            for metric in ['eval_accuracy', 'eval_f1', 'eval_bleu']:\n                if metric in eval_results:\n                    print(f\"   {metric.replace('eval_', '').title()}: {eval_results[metric]:.4f}\")\n        else:\n            print(\"   Evaluation results format unexpected\")\n    else:\n        print(\"❌ Evaluation not performed\")\nexcept Exception as e:\n    print(f\"❌ Evaluation check failed: {str(e)}\")\n\n# Check model saving\ntry:\n    import os\n    local_path = \"/kaggle/working/llama-3b-medical-cot\"\n    if os.path.exists(local_path):\n        print(f\"✅ Model saved locally to {local_path}\")\n        # Check if both model and tokenizer files exist\n        model_files = os.listdir(local_path)\n        if any('pytorch_model' in f or 'model.safetensors' in f for f in model_files):\n            print(\"   ✅ Model files found\")\n        if any('tokenizer' in f for f in model_files):\n            print(\"   ✅ Tokenizer files found\")\n    else:\n        print(\"❌ Local model save directory not found\")\nexcept Exception as e:\n    print(f\"❌ Model save check failed: {str(e)}\")\n\n# Check HuggingFace token\ntry:\n    if 'hf_token' in locals() and hf_token is not None:\n        print(\"✅ HuggingFace token loaded\")\n        print(\"✅ Ready for HuggingFace upload!\")\n    else:\n        print(\"⚠️  HuggingFace token not found - upload may fail\")\nexcept Exception as e:\n    print(f\"❌ HuggingFace token check failed: {str(e)}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Training pipeline status check complete!\")\n\n# Overall success check\nall_components = [\n    'train_formatted' in locals() and len(train_formatted) > 0,\n    'model' in locals() and model is not None,\n    'tokenizer' in locals() and tokenizer is not None,\n    'train_result' in locals() and train_result is not None,\n    'eval_results' in locals() and eval_results is not None\n]\n\nif all(all_components):\n    print(\"🎉 All components successful - Model is ready for deployment!\")\nelse:\n    failed_components = []\n    component_names = ['Dataset', 'Model', 'Tokenizer', 'Training', 'Evaluation']\n    for i, success in enumerate(all_components):\n        if not success:\n            failed_components.append(component_names[i])\n    print(f\"⚠️  Some components failed: {', '.join(failed_components)}\")\n    print(\"   Please check the errors above before proceeding.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T07:41:27.977214Z","iopub.execute_input":"2025-05-31T07:41:27.977510Z","iopub.status.idle":"2025-05-31T07:41:27.999137Z","shell.execute_reply.started":"2025-05-31T07:41:27.977492Z","shell.execute_reply":"2025-05-31T07:41:27.998331Z"}},"outputs":[],"execution_count":null}]}